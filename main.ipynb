{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"shba93/tim-rec\")[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Densify index: create a dictionary to store the mapping between original values and densified values\n",
    "mapping = {}\n",
    "\n",
    "for var_name in [\"user_id\", \"item_id\"]:\n",
    "    # Create a mapping from original values to dense indices\n",
    "    mapping[var_name] = {u: i + 1 for i, u in enumerate(set(dataset[var_name]))}  # Probably not a great way to name maps\n",
    "\n",
    "    # Update the dataframe column with densified indices using the created mapping\n",
    "    dataset[var_name] = dataset[var_name].map(mapping[var_name])\n",
    "dataset[\"contact_outcome\"] = dataset[\"contact_outcome\"].map({\"Accepted\": 1, \"Refused\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(mapping[\"user_id\"])\n",
    "num_items = len(mapping[\"item_id\"])\n",
    "print(num_users, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to sequences.\n",
    "df_group_by_user = dataset.groupby(\"user_id\")\n",
    "\n",
    "data = {}\n",
    "data[\"item_id\"] = df_group_by_user.apply(lambda d: list(d.sort_values(by=\"contact_date\")[\"item_id\"])).values\n",
    "data[\"contact_outcome\"] = df_group_by_user.apply(lambda d: list(d.sort_values(by=\"contact_date\")[\"contact_outcome\"])).values\n",
    "data[\"user_id\"] = df_group_by_user.apply(lambda d: list(d[\"user_id\"])[0]).values #0 cause variable should be the same for every entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percentage = 0.2\n",
    "end_ids = np.array([len(seq) for seq in data[\"item_id\"]])\n",
    "test_size = np.ceil(test_percentage*end_ids).astype(int)\n",
    "end_ids -= test_size\n",
    "data[\"train_item_id\"] = np.array([seq[:end_ids[i]] for i,seq in enumerate(data[\"item_id\"])], dtype=object)\n",
    "data[\"test_item_id\"] = np.array([seq[end_ids[i]:] for i,seq in enumerate(data[\"item_id\"])], dtype=object)\n",
    "data[\"train_contact_outcome\"] = np.array([seq[:end_ids[i]] for i,seq in enumerate(data[\"contact_outcome\"])], dtype=object)\n",
    "data[\"test_contact_outcome\"] = np.array([seq[end_ids[i]:] for i,seq in enumerate(data[\"contact_outcome\"])], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ratings matrix\n",
    "R = {}\n",
    "for split_name in [\"train\",\"test\"]:\n",
    "    R[split_name] = scipy.sparse.lil_matrix((num_users+1, num_items+1))\n",
    "    for s, u, r in zip(data[f\"{split_name}_item_id\"], data[f\"user_id\"], data[f\"{split_name}_contact_outcome\"]):\n",
    "        # if split_name == \"test\": #remove training data\n",
    "        #     l = len(data[\"train_sid\"][u-1])\n",
    "        #     s = s[l:]\n",
    "        #     r = r[l:]\n",
    "        for sid, rating in zip(s, r):\n",
    "            R[split_name][u, sid] = (rating - 0.5) * 2 #convert to -1,1\n",
    "    R[split_name] = R[split_name][1:, 1:] #remove user 0 and item 0 (which are not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_at_least_one_acceptance = set()\n",
    "for u in range(num_users):\n",
    "    if np.sum(R[\"test\"][u, R[\"test\"][u].nonzero()[1]])>0:\n",
    "        users_with_at_least_one_acceptance.add(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = scipy.sparse.linalg.svds(R[\"train\"], k=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for each user\n",
    "pred = np.dot(U, np.dot(np.diag(S), Vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    # Order each row\n",
    "    pred_items_idx = np.argsort(-pred, axis=1)\n",
    "    ranks = np.argsort(pred_items_idx, axis=1) + 1\n",
    "\n",
    "    # For each user, compute the NDCG, Precision and Recall of the top-k recommendations\n",
    "    users_sets = {\n",
    "        \"all\": range(num_users),\n",
    "        \"at_least_one_acceptance\": users_with_at_least_one_acceptance\n",
    "    }\n",
    "    metrics = {}\n",
    "\n",
    "    for users_set_name,users_set_to_use in users_sets.items():\n",
    "        for k in [1,5,10,20]:\n",
    "            metrics[f\"{users_set_name}_Precision@{k}\"] = []\n",
    "            metrics[f\"{users_set_name}_Recall@{k}\"] = []\n",
    "            metrics[f\"{users_set_name}_NDCG@{k}\"] = []\n",
    "            metrics[f\"{users_set_name}_NegNDCG@{k}\"] = []\n",
    "        for u in tqdm.tqdm(users_set_to_use):\n",
    "            # Get the ground truth\n",
    "            ground_truth_items = R[\"test\"][u].nonzero()[1]\n",
    "\n",
    "            for k in [1,5,10,20]:\n",
    "                # Get the top-k recommendations\n",
    "                top_k_recs = pred_items_idx[u, :k]\n",
    "\n",
    "                # Compute Precision\n",
    "                metrics[f\"{users_set_name}_Precision@{k}\"].append(len(set(top_k_recs).intersection(set(ground_truth_items))) / k)\n",
    "\n",
    "                # Compute Recall\n",
    "                metrics[f\"{users_set_name}_Recall@{k}\"].append(len(set(top_k_recs).intersection(set(ground_truth_items))) / len(ground_truth_items))\n",
    "\n",
    "                # Compute NDCG\n",
    "                relevance = R[\"test\"][u, top_k_recs].toarray().flatten()\n",
    "                dcg = np.sum(relevance / np.log2(ranks[u, top_k_recs] + 1))\n",
    "                all_relevance = R[\"test\"][u].toarray().flatten()\n",
    "                sorted_relevance = np.sort(all_relevance)\n",
    "                idcg = np.sum(sorted_relevance[::-1][:k] / np.log2(np.arange(2, k + 2)))\n",
    "                ndcg = dcg / idcg if idcg != 0 else 0\n",
    "                metrics[f\"{users_set_name}_NDCG@{k}\"].append(ndcg)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics):\n",
    "    for key,values in metrics.items():\n",
    "        print(f\"{key}: {np.mean(values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
